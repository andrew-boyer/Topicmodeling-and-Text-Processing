#multiple lines read in csv
Contivation <- function(words, num.topics){
  library(topicmodels)
  library(dplyr)
  library(tidytext)
  library(tm)
  library(ggplot2)
  library(tidyr)
  library(stringr)
  library(data.table)
  #library(wordcloud)
  
  ####################
  ##Timing Function ##
  ####################
  start.time <- Sys.time()
  
  ################################
  ##  Loading the Initial Data  ##
  ################################
  myfile <- file.choose()
  data <- fread(myfile, header = FALSE, sep = "\n", blank.lines.skip = TRUE)
  #n <- nrow(data) #essentially counting the number of rows in the file
  names(data)[1] <- "text" #change column name
  data <- tibble::rowid_to_column(data, "doc_id") #add doc_id to table so dataframesource will function correctly
  
  (ds <- DataframeSource(data))
  x <- Corpus(ds)#creating corpus
  
  #######################################################################################
  ##                                                                                  ###
  ##    Cleaning up text and removing unncessary words with the tm_map function       ###
  ##                                                                                  ###
  #######################################################################################
  
  # Convert the text to lower case
  x <- tm_map(x, content_transformer(tolower))
  #remove numbers if they are there
  x <- tm_map(x, removeNumbers)
  # Remove english common stopwords
  x <- tm_map(x, removeWords, stopwords("english"))
  # specify your stopwords as a character vector
  x <- tm_map(x, removeWords, c("said", "one", "year")) 
  # Remove punctuations
  x <- tm_map(x, removePunctuation, preserve_intra_word_dashes = TRUE)
  #Remove Numbers
  x <- tm_map(x, removeNumbers)
  #Stem words
  x <- tm_map(x, stemDocument, language = "english")
  # Eliminate extra white spaces
  x <- tm_map(x, stripWhitespace)
  
  
  ### Convert to Document Term Matrix and remove empty documents ###
  min.freq <- 4 #setting minimum frequency to make LDA process faster
  dtm <- DocumentTermMatrix(x)
  ui <- unique(dtm$i)
  dtm.new <- dtm[ui,]
  
  ###   ###  ###   ###   ###   ###  ####  ##
  ## Creating LDA for language processing ##
  ###   ###  ###   ###   ###   ###  ####  ##
  x_lda <- LDA(dtm.new, k=num.topics, control = list(seed = 1234)) #k is the number of TOPICS i.e. the variable they are looking for

  x_topics <- tidy(x_lda, matrix = "beta") #per-topic-per-word probility
  #x_topicsg <- tidy(x_lda, matrix = "gamma")#examine the per-document-per-topic probabilities
  ## Right now the per document per topic probability is not used but it     ##
  ## could be implemented to determine which topic either document belongs   ##
  
  
  ###   ###  ###   ###   ###   ###  #### ###  ###   ###   ###   ##
  # code below chooses the top n terms as specified by the user ##
  ###   ###  ###   ###   ###   ###  #### ###  ###   ###   ###   ##
  x_output <- x_topics %>%
    group_by(topic) %>%
    top_n(words, beta) %>%# the variable "words" allows the user to specify the number of words printed
    ungroup() %>%
    arrange(topic, -beta)

  #print(x_output)
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  print(time.taken)
  
  View(x_output)
  return(x_output)
  

}

